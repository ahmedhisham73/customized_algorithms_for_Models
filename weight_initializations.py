# -*- coding: utf-8 -*-
"""weight_initializations.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1KdMBYvWLYGgsJQv6k8qFFff9DGkJlGEI
"""

import tensorflow as tf
from tensorflow.keras import layers
from tensorflow.keras import initializers

layer = layers.Dense(
    units=64,
    kernel_initializer=initializers.RandomNormal(stddev=0.01),
    bias_initializer=initializers.Zeros()
)

#Initializer that generates tensors with a normal distribution.

# Zero Initialization
from tensorflow.keras import layers
from tensorflow.keras import initializers
 
initializer = tf.keras.initializers.Zeros()
layer = tf.keras.layers.Dense(
  3, kernel_initializer=initializer)

"""In an attempt to overcome the shortcomings of Zero or Constant Initialization,
 random initialization assigns random values except for zeros as weights to neuron paths.
  However, assigning values randomly to the weights, problems such as Overfitting, Vanishing Gradient Problem,
   Exploding Gradient Problem might occur.

Random Initialization can be of two kinds:

1-Random Normal
2-Random Uniform


Random Normal: The weights are initialized from values in a normal distribution.
Random Uniform: The weights are initialized from values in a uniform distribution.
"""

# Random Normal Distribution
from tensorflow.keras import layers
from tensorflow.keras import initializers

initializer = tf.keras.initializers.RandomNormal(
mean=0., stddev=1.)
layer = tf.keras.layers.Dense(3, kernel_initializer=initializer)

# Random Uniform Initialization
from tensorflow.keras import layers
from tensorflow.keras import initializers

initializer = tf.keras.initializers.RandomUniform(
minval=0.,maxval=1.)
layer = tf.keras.layers.Dense(3, kernel_initializer=initializer)

"""
VIIIIIIIIIII
Xavier/Glorot Initialization
In Xavier/Glorot weight initialization, the weights are assigned from values of a uniform distribution 
Xavier/Glorot Initialization often termed as Xavier Uniform Initialization, 
is suitable for layers where the activation function used is Sigmoid. 
"""

# Xavier/Glorot Uniform Initialization
from tensorflow.keras import layers
from tensorflow.keras import initializers

initializer = tf.keras.initializers.GlorotUniform()
layer = tf.keras.layers.Dense(3, kernel_initializer=initializer)

"""
In Normalized Xavier/Glorot weight initialization, the weights are assigned from values of a normal distribution """

# Normailzed Xavier/Glorot Uniform Initialization
from tensorflow.keras import layers
from tensorflow.keras import initializers

initializer = tf.keras.initializers.GlorotNormal()
layer = tf.keras.layers.Dense(3, kernel_initializer=initializer)

"""
In He Uniform weight initialization, the weights are assigned from values of a uniform distribution 
He Uniform Initialization is suitable for layers where ReLU activation function is used. 


"""

# He Uniform Initialization
from tensorflow.keras import layers
from tensorflow.keras import initializers

initializer = tf.keras.initializers.HeUniform()
layer = tf.keras.layers.Dense(3, kernel_initializer=initializer)

"""
# He Normal Initialization
from tensorflow.keras import layers
from tensorflow.keras import initializers

initializer = tf.keras.initializers.HeNormal()
layer = tf.keras.layers.Dense(3, kernel_initializer=initializer)
"""

# He Normal Initialization
from tensorflow.keras import layers
from tensorflow.keras import initializers

initializer = tf.keras.initializers.HeNormal()
layer = tf.keras.layers.Dense(3, kernel_initializer=initializer)

