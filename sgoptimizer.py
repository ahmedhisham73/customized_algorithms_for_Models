# -*- coding: utf-8 -*-
"""SGOptimizer.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1z3D5fflylG8Vi4zoXpv_ulmrY_4D_Amm
"""

import tensorflow as tf
from tensorflow import keras

# Common imports
import numpy as np
import os

from sklearn.datasets import fetch_california_housing
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

housing = fetch_california_housing()
X_train_full, X_test, y_train_full, y_test = train_test_split(
    housing.data, housing.target.reshape(-1, 1), random_state=42)
X_train, X_valid, y_train, y_valid = train_test_split(
    X_train_full, y_train_full, random_state=42)

scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_valid_scaled = scaler.transform(X_valid)
X_test_scaled = scaler.transform(X_test)

'''
we want an optimizer such that this optimizer can do the following
mimic the gradient descent algorithim by applying the formula

w=w-alpha*dw
b=b-alpha*db
w is the weight variable 
b is the bias variable 
dw: weight update gradient 
alpha : learning rate 
db: bias update gradient


here we want an optimizer to generalize this formula 

'''

class SGOptimizer(keras.optimizers.Optimizer):
    def __init__(self,learning_rate=0.01,name="SGOptimizer",**kwargs):
        """Call super().__init__() and use _set_hyper() to store hyperparameters"""
        super().__init__(name,**kwargs)
        self._set_hyper("learning_rate", kwargs.get("lr", learning_rate))
        self._is_first = True

    def _create_slots(self, var_list):
         
        for var in var_list:
            self.add_slot(var, "pv") #previous variable i.e. weight or bias
        for var in var_list:
            self.add_slot(var, "pg") #previous gradient


    @tf.function
    def _resource_apply_dense(self, grad, var):
        """Update the slots and perform one optimization step for one model variable
        """
        var_dtype = var.dtype.base_dtype
        lr_t = self._decayed_lr(var_dtype) # handle learning rate decay
        #w=w-alpha*dw
        new_var_m = var - grad * lr_t
        pv_var = self.get_slot(var, "pv")
        pg_var = self.get_slot(var, "pg")
        
        if self._is_first:
            self._is_first = False
            new_var = new_var_m
        else:
            cond = grad*pg_var >= 0
            print(cond)
            avg_weights = (pv_var + var)/2.0
            new_var = tf.where(cond, new_var_m, avg_weights)
        pv_var.assign(var)
        pg_var.assign(grad)
        var.assign(new_var)



    def _resource_apply_sparse(self, grad, var):
        raise NotImplementedError

    def get_config(self):
        base_config = super().get_config()
        return {
            **base_config,
            "learning_rate": self._serialize_hyperparameter("learning_rate"),
        }

keras.backend.clear_session()
np.random.seed(42)
tf.random.set_seed(42)

model = keras.models.Sequential([keras.layers.Dense(1, input_shape=[8])])
model.compile(loss="mse", optimizer=SGOptimizer(learning_rate=0.001))
model.fit(X_train_scaled, y_train, epochs=50)

